{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Database from Web Scraping from the IMDB Website"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We walk through step-by-step how we went about webscraping through the IMDB website with Beautiful Soup.  We import the following modules for webscraping: requests, time, random, and tqdm.  We import the following modules for data cleaning:  numpy and pandas.  Requests is the module we use for send the http requests to get the HTML files, which we use the module BeautifulSoup to parse.  Numpy is the module we use for certain mathematical operations and array manipulation.  Pandas is the module for organizing the data into a DataFrame, which facilitates data cleaning and analysis.  Time is a module for representing time in code and allows us to create pauses.during code execution, while random helps us to create pseudo-random numbers for the pauses in the code execution.  Tqdm creates the progress bar for executing code for API calling or webscraping.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import time\n",
    "import random\n",
    "from tqdm.notebook import trange, tqdm\n",
    "from bs4 import BeautifulSoup as BS\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Scraping through One Entry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to walk through the process of working through a single page before the process of applying the same process to multiple pages.  We start with the URL that we want to scrape.  We make a GET request from the API of the website, and we receive a Response object in return.  We pass the response into the BeautifulSoup constructor, which parses the document with the HTML parser.  We are then able to view the HTML document as a complex tree of Python objects.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ipt crossorigin=\"anonymous\" src=\"https://m.media-amazon.com/images/I/21a9eB+eAFL.js\" type=\"text/javascript\">\n",
      "  </script>\n",
      "  <script crossorigin=\"anonymous\" src=\"https://m.media-amazon.com/images/I/412kU3nfA9L.js\" type=\"text/javascript\">\n",
      "  </script>\n",
      "  <script crossorigin=\"anonymous\" src=\"https://m.media-amazon.com/images/I/81TI9nS4vuL.js\" type=\"text/javascript\">\n",
      "  </script>\n",
      "  <script crossorigin=\"anonymous\" src=\"https://m.media-amazon.com/images/I/61vDTVmJCNL.js\" type=\"text/javascript\">\n",
      "  </script>\n",
      "  <script crossorigin=\"anonymous\" src=\"https://m.media-amazon.com/images/I/31827uXCh4L.js\" type=\"text/javascript\">\n",
      "  </script>\n",
      "  <script crossorigin=\"anonymous\" src=\"https://m.media-amazon.com/images/I/317ZcIHzftL.js\" type=\"text/javascript\">\n",
      "  </script>\n",
      "  <script crossorigin=\"anonymous\" src=\"https://m.media-amazon.com/images/I/41Dm6cYzV6L.js\" type=\"text/javascript\">\n",
      "  </script>\n",
      "  <script crossorigin=\"anonymous\" src=\"https://m.media-amazon.com/images/I/51GDom0+d0L.js\" type=\"text/javascript\">\n",
      "  </script>\n",
      "  <script crossorigin=\"anonymous\" src=\"https://m.media-amazon.com/images/I/01ZyMmZoX7L.js\" type=\"text/javascript\">\n",
      "  </script>\n",
      "  <script crossorigin=\"anonymous\" src=\"https://m.media-amazon.com/images/I/11UNuUz7BzL.js\" type=\"text/javascript\">\n",
      "  </script>\n",
      "  <script crossorigin=\"anonymous\" src=\"https://m.media-amazon.com/images/I/21n5fdlWBhL.js\" type=\"text/javascript\">\n",
      "  </script>\n",
      "  <script crossorigin=\"anonymous\" src=\"https://m.media-amazon.com/images/I/01X4+ME2ObL.js\" type=\"text/javascript\">\n",
      "  </script>\n",
      "  <script crossorigin=\"anonymous\" src=\"https://m.media-amazon.com/images/I/619IuBSowVL.js\" type=\"text/javascript\">\n",
      "  </script>\n",
      "  <script>\n",
      "   if (typeof uet == 'function') {\n",
      "      uet(\"be\", \"LoadFooterJS\", {wb: 1});\n",
      "    }\n",
      "  </script>\n",
      "  <script>\n",
      "   if (typeof uex == 'function') {\n",
      "      uex(\"ld\", \"LoadFooterJS\", {wb: 1});\n",
      "    }\n",
      "  </script>\n",
      "  <div id=\"servertime\" time=\"180\">\n",
      "  </div>\n",
      "  <script>\n",
      "   if (typeof uet == 'function') {\n",
      "      uet(\"be\");\n",
      "    }\n",
      "  </script>\n",
      " </body>\n",
      "</html>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "url = 'https://www.imdb.com/search/title/?groups=top_1000&sort=boxoffice_gross_us,desc'\n",
    "response = requests.get(url)\n",
    "soup = BS(response.text, \"html.parser\")\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We identify the div class of lister-item mode-advanced to contain the information that we need.  We use the find() method to extract the first of the 50 div containers representing the top 50 of the 1000 on the webpage.  We try to put out the value of just one to make sure we have the right values.  We are grabbing movie titles, IMDB id's, PG ratings, the year of release, runtimes, genres, IMDB ratings, metascores, total number of votes submitted for IMDB rating, and gross US earnings.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "div = soup.find('div', {'class':'lister-item mode-advanced'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Star Wars: Episode VII - The Force Awakens\n",
      "tt2488496\n",
      "(2015)\n",
      "PG-13\n",
      "138 min\n",
      "\n",
      "Action, Adventure, Sci-Fi            \n",
      "7.9\n",
      "80        \n",
      "850,565\n",
      "$936.66M\n"
     ]
    }
   ],
   "source": [
    "title = div.h3.a.text\n",
    "imdb_id = div.find('img')['data-tconst']\n",
    "year = div.h3.find('span', class_='lister-item-year').text\n",
    "pg_rating = div.find('span', class_='certificate').text\n",
    "runtime = div.find('span', class_='runtime').text\n",
    "genre = div.find('span', class_='genre').text\n",
    "imdb_rating = div.strong.text\n",
    "metascore = div.find('span', class_='metascore').text\n",
    "nv = div.find_all('span', attrs={'name': 'nv'})\n",
    "vote = nv[0].text\n",
    "gross = nv[1].text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Scraping through Multiple Entries on Multiple Pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to iterate through all 50 instances and then to iterate through all 200 pages for the 1000 entries, we create a for loop to iterate through the 50 movies on each page and another for loop to iterate through each page.  First, we instantiate the variables that will represent each of the data we are gathering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = []\n",
    "imdb_ids = []\n",
    "pg_ratings = []\n",
    "years = []\n",
    "runtimes = []\n",
    "genres = []\n",
    "imdb_ratings = []\n",
    "metascores = []\n",
    "votes = []\n",
    "gross_us = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To iterate through each page, we look at the URLs for each page containing 50 entries and find the pattern for iteration.  As we move past the 1st page through the pages, the value after start changes, from 51, 101, 151, 201, etc.  We postulate that if we substitute 1 we would get the original page.  We use string formatting to insert the different values we are iterating through.  We are iterating starting the range of 1 to 1000 at intervals of 50.  As we find the values, we append them to their respective lists.  When we started to run the for loops, we encountered errors.  We added to some of the original code to account for empty values to ensure all of our lists had 1000 values at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0221b39610e64c64962c6ebb345fe54d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Download Progress:'), FloatProgress(value=0.0, max=20.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for n in tqdm(range(1, 1000, 50), desc='Download Progress:'):\n",
    "    # Create an expression that represents each page for the iteration\n",
    "    url = 'https://www.imdb.com/search/title/?groups=top_1000&sort=boxoffice_gross_us,desc&start={}&ref_=adv_nxt'.format(n)\n",
    "    # The requests library makes a get request to the url for data, which is saved to results\n",
    "    results = requests.get(url)\n",
    "    # Create an instance of BeautifulSoup to parse results\n",
    "    soup = BS(results.text, \"html.parser\")\n",
    "    # Find the div container in the HTML that contains the wanted information.  \n",
    "    movie_div = soup.find_all('div', class_='lister-item mode-advanced')\n",
    "    # Use function to suspend execution of calling thread at random\n",
    "    time.sleep(random.randint(3, 12))\n",
    "\n",
    "    # for each container in the div container created above by Beautiful Soup\n",
    "    for div in movie_div:\n",
    "        # to get the titles, we use attribute notation to access the title contained as text in the <a> tag nested inside the <h3> tag \n",
    "        # dot notation only works with the first instance of the tag\n",
    "        title = div.h3.a.text\n",
    "        # we append the scraped title to the titles list through each iteration\n",
    "        titles.append(title)\n",
    "        # to get the IMDB id's, we use the find() method to find the first instance \n",
    "        imdb_id = div.find('img')['data-tconst']\n",
    "        imdb_ids.append(imdb_id)\n",
    "        # to scrape the year, we use the find() method to search nested inside the h3 tag to get the text inside the span tag with the class lister-item-year\n",
    "        year = div.h3.find('span', class_='lister-item-year').text\n",
    "        years.append(year)\n",
    "        # to scrape the pg rating, we use the find method again but create a condition in the case of blank values\n",
    "        pg_rating = div.find('span', class_='certificate').text if div.p.find('span', class_='certificate') else '--'\n",
    "        pg_ratings.append(pg_rating)\n",
    "        # to scrape the runtime, we use a similar method from above\n",
    "        runtime = div.find('span', class_='runtime').text if div.p.find('span', class_='runtime') else '--'\n",
    "        runtimes.append(runtime)\n",
    "        # to scrape the genres, we employed the same method\n",
    "        genre = div.find('span', class_='genre').text\n",
    "        genres.append(genre)\n",
    "        # to scrape the IMDB rating, we call the distinctive strong tag which wraps the desired text\n",
    "        imdb_rating = div.strong.text\n",
    "        imdb_ratings.append(imdb_rating)\n",
    "        # to scrape the etascore, we use a similar code as above\n",
    "        metascore = div.find('span', class_='metascore').text if div.find('span', class_='metascore') else '--'\n",
    "        metascores.append(metascore)\n",
    "        # to scrape the votes and gross us earnings, we use the find_all method, which finds all the instances of the span tag with the name attribute and value of nv\n",
    "        # if there is one item in the list, it represents the vote, and if there are two items, then we get the gross earning and return a string if it's empty to ensure all our lists are the same length to make the DataFrame\n",
    "        nv = div.find_all('span', attrs={'name': 'nv'})\n",
    "        vote = nv[0].text\n",
    "        votes.append(vote)\n",
    "        gross = nv[1].text if len(nv) > 1 else '--'\n",
    "        gross_us.append(gross)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating DataFrame for Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then created a dataframe that would contain all the values of the lists with each column representing a different type of data.  This allows us to clean and manipulate the data more easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie</th>\n",
       "      <th>year</th>\n",
       "      <th>pg_rating</th>\n",
       "      <th>imdb_id</th>\n",
       "      <th>runtime</th>\n",
       "      <th>genre</th>\n",
       "      <th>metascore</th>\n",
       "      <th>imdb_rating</th>\n",
       "      <th>votes</th>\n",
       "      <th>gross_us</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Star Wars: Episode VII - The Force Awakens</td>\n",
       "      <td>(2015)</td>\n",
       "      <td>PG-13</td>\n",
       "      <td>tt2488496</td>\n",
       "      <td>138 min</td>\n",
       "      <td>\\nAction, Adventure, Sci-Fi</td>\n",
       "      <td>80</td>\n",
       "      <td>7.9</td>\n",
       "      <td>850,565</td>\n",
       "      <td>$936.66M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Avengers: Endgame</td>\n",
       "      <td>(2019)</td>\n",
       "      <td>PG-13</td>\n",
       "      <td>tt4154796</td>\n",
       "      <td>181 min</td>\n",
       "      <td>\\nAction, Adventure, Drama</td>\n",
       "      <td>78</td>\n",
       "      <td>8.4</td>\n",
       "      <td>778,142</td>\n",
       "      <td>$858.37M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Avatar</td>\n",
       "      <td>(2009)</td>\n",
       "      <td>PG-13</td>\n",
       "      <td>tt0499549</td>\n",
       "      <td>162 min</td>\n",
       "      <td>\\nAction, Adventure, Fantasy</td>\n",
       "      <td>83</td>\n",
       "      <td>7.8</td>\n",
       "      <td>1,108,161</td>\n",
       "      <td>$760.51M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Avengers: Infinity War</td>\n",
       "      <td>(2018)</td>\n",
       "      <td>PG-13</td>\n",
       "      <td>tt4154756</td>\n",
       "      <td>149 min</td>\n",
       "      <td>\\nAction, Adventure, Sci-Fi</td>\n",
       "      <td>68</td>\n",
       "      <td>8.4</td>\n",
       "      <td>811,463</td>\n",
       "      <td>$678.82M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Titanic</td>\n",
       "      <td>(1997)</td>\n",
       "      <td>PG-13</td>\n",
       "      <td>tt0120338</td>\n",
       "      <td>194 min</td>\n",
       "      <td>\\nDrama, Romance</td>\n",
       "      <td>75</td>\n",
       "      <td>7.8</td>\n",
       "      <td>1,031,548</td>\n",
       "      <td>$659.33M</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        movie    year pg_rating    imdb_id  \\\n",
       "0  Star Wars: Episode VII - The Force Awakens  (2015)     PG-13  tt2488496   \n",
       "1                           Avengers: Endgame  (2019)     PG-13  tt4154796   \n",
       "2                                      Avatar  (2009)     PG-13  tt0499549   \n",
       "3                      Avengers: Infinity War  (2018)     PG-13  tt4154756   \n",
       "4                                     Titanic  (1997)     PG-13  tt0120338   \n",
       "\n",
       "   runtime                                     genre   metascore imdb_rating  \\\n",
       "0  138 min   \\nAction, Adventure, Sci-Fi              80                 7.9   \n",
       "1  181 min    \\nAction, Adventure, Drama              78                 8.4   \n",
       "2  162 min  \\nAction, Adventure, Fantasy              83                 7.8   \n",
       "3  149 min   \\nAction, Adventure, Sci-Fi              68                 8.4   \n",
       "4  194 min              \\nDrama, Romance              75                 7.8   \n",
       "\n",
       "       votes  gross_us  \n",
       "0    850,565  $936.66M  \n",
       "1    778,142  $858.37M  \n",
       "2  1,108,161  $760.51M  \n",
       "3    811,463  $678.82M  \n",
       "4  1,031,548  $659.33M  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a Pandas dataframe with the \n",
    "movies = pd.DataFrame({\n",
    "    'movie': titles, 'year' : years, 'pg_rating' : pg_ratings, 'imdb_id' : imdb_ids, 'runtime' : runtimes, 'genre' : genres, 'metascore' : metascores, 'imdb_rating' : imdb_ratings, 'votes' : votes, 'gross_us' : gross_us\n",
    "})\n",
    "# Look at the first few columns to get an idea of how to clean the data\n",
    "movies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'80        '"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies['metascore'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'7.9'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies['imdb_rating'][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'850,565'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies['votes'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nAction, Adventure, Sci-Fi            '"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies['genre'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We used extract and regex to pull out any digits of length 1 or more and to turn that string into an integer type\n",
    "movies['imdb_id'] = movies['imdb_id'].str.extract('(\\d+)').astype(int)\n",
    "movies['year'] = movies['year'].str.extract('(\\d+)').astype(int)\n",
    "movies['runtime'] = movies['runtime'].str.extract('(\\d+)').astype(int)\n",
    "# We used rstrip() to remove whitespace right of the metascore\n",
    "movies['metascore'] = movies['metascore'].str.rstrip()\n",
    "# Because of empty values, we can convert those non-numeric values to numeric with coerce\n",
    "movies['metascore'] = pd.to_numeric(movies['metascore'], errors='coerce')\n",
    "# We used replace() to remove the comma from the vote count and turn the value into an integer\n",
    "movies['votes'] = movies['votes'].str.replace(',', '').astype(int)\n",
    "# We used map() and lambda function to apply the lstrip() method and rstrip() method\n",
    "movies['gross_us'] = movies['gross_us'].map(lambda x: x.lstrip('$').rstrip('M'))\n",
    "# We used map() and lambda function to strip \\n in front and whitespace after\n",
    "movies['genre'] = movies['genre'].map(lambda x: x.strip('\\n').rstrip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write dataframe to a CSV file\n",
    "movies.to_csv('top_1000_by_us_box_office.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating database from API requests on TMDB website\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We collected another data set from TMDB using their sample URL requests tool to create the appropriate URL to find the top 1000 most popular movies.  The code is very similar to the webscraping one, except that we create a request using the Request() constructor to fetch a .json file.  We call the json() method to read and parse the data into a dictionary.  The for loop iterates through each of the pages and appends the data into compiled_list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making API Call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compiled_list = []\n",
    "\n",
    "for n in tqdm(list(range(1,399))):\n",
    "    url='https://api.themoviedb.org/3/movie/top_rated?api_key=0e72c0b2b11293a6390e9f7b472aec2b&language=en-US&page={}'.format(n)\n",
    "    r = requests.get(url)\n",
    "    data = r.json()\n",
    "    compiled_list.append(data)\n",
    "    time.sleep(random.choice([1,2]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = []\n",
    "for page in compiled_list:\n",
    "    movies += page['results']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating DataFrame for Data Manipulation and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(movies)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon inspecting the data, we realized we had to convert the genre ID's into the actual genre names.  We achieved that by creating dictionary of key:value pairs, where the key is the ID and value is the genre name.  We passed the dictionary through each of the rows of the genre column to replace the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genre = {28: 'Action', 12: \"Adventure\", 16: 'Animation', 35: 'Comedy', 80: 'Crime', 99: 'Documentary', 18: 'Drama', 10751: 'Family', 14: 'Fantasy', 36: 'History', 27: 'Horror', 10402: 'Music', 9648: 'Mystery', 10749: 'Romance', 878: 'Science Fiction', 10770: 'TV Movie', 53: 'Thriller', 10752: 'War', 37: 'Western'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['genre_ids'] = df['genre_ids'].apply(lambda g_id: [genre[g] for g in g_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We uploaded the CSV from the webscraping exercise., merged the two dataframes together, and inspected our resulting dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top = pd.read_csv('top_1000_by_us_box_office.csv')\n",
    "combined = pd.merge(df, top, left_on = 'title', right_on = 'movie')\n",
    "combined.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We checked for any null values in the table, and dropped the rows which had null values for the gross earnings.  We also wanted to pull the month out of the release date, so we could sort by month.  By checking dtypes, we saw some columns needed to be converted into integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined['month'] = combined['release_date'].apply(lambda m: m[5:7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined['year'] = combined['year'].apply(lambda x: int(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined['month'] = combined['month'].apply(lambda x: int(x))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
